# 决策树资料收集



## 决策树的基础

可参考文档:https://blog.csdn.net/jiaoyangwm/article/details/79525237 

![image-20201108105021486](D:\技能学习\bigdata\img\决策树(CSDN).png)

### 1.信息熵，信息增益，信息增益率，基尼杂质(K3,M3)

参考文档:

纯理论

https://blog.csdn.net/ywx1832990/article/details/81539347?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.pc_relevant_is_cache&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.pc_relevant_is_cache



有计算案例的：

https://blog.csdn.net/PIPIXIU/article/details/82980740?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.pc_relevant_is_cache&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.pc_relevant_is_cache



? 香浓定理



- 信息熵

  ![image-20201108151703843](D:\技能学习\bigdata\img\决策树资料收集\image-20201108151703843.png)

- 熵的混乱程度

![image-20201108154802939](D:\技能学习\bigdata\img\决策树资料收集\image-20201108154802939.png)





- 计算信息熵

![image-20201108151829456](D:\技能学习\bigdata\img\决策树资料收集\image-20201108151829456.png)

- 条件熵

![img](D:\技能学习\bigdata\img\决策树资料收集\SNAGHTML61defd2.PNG)

- 条件熵的计算公式

  ![image-20201108161344973](D:\技能学习\bigdata\img\决策树资料收集\image-20201108161344973.png)

- 信息增益-- 熵与条件熵的差值

  gain(D,A) = H(D) - H(D|A)  

  ![image-20201108161702121](D:\技能学习\bigdata\img\决策树资料收集\image-20201108161702121.png)

  如果在已知特征A后，不确定性下降的特别多，那么就证明特征A是非常有用的；反之，如果不确定程度没有下降，

  那么就证明特征A是没有什么用的，H(D)约等于H(D|A),信息增益几乎约等于0

- ID3

  - 具体步骤： 

    ![image-20201108162314243](D:\技能学习\bigdata\img\决策树资料收集\image-20201108162314243.png)

    如果有三个特征A，B，C，那么就要分别算出g(D,A) g(D,B) g(D,C) 取最大的作为分裂节点

  - ID3的局限性

    ![image-20201108164006978](D:\技能学习\bigdata\img\决策树资料收集\image-20201108164006978.png)

  - 计算案例

    ![image-20201108194423995](D:\技能学习\bigdata\img\决策树资料收集\image-20201108194423995.png)

- 信息增益率

  | Day   | Temperatrue | Outlook  | Humidity | Windy | PlayGolf? |
  | :---- | :---------- | :------- | :------- | :---- | :-------- |
  | 07-05 | hot         | sunny    | high     | false | no        |
  | 07-06 | hot         | sunny    | high     | true  | no        |
  | 07-07 | hot         | overcast | high     | false | yes       |
  | 07-09 | cool        | rain     | normal   | false | yes       |
  | 07-10 | cool        | overcast | normal   | true  | yes       |
  | 07-12 | mild        | sunny    | high     | false | no        |
  | 07-14 | cool        | sunny    | normal   | false | yes       |
  | 07-15 | mild        | rain     | normal   | false | yes       |
  | 07-20 | mild        | sunny    | normal   | true  | yes       |
  | 07-21 | mild        | overcast | high     | true  | yes       |
  | 07-22 | hot         | overcast | normal   | false | yes       |
  | 07-23 | mild        | sunny    | high     | true  | no        |
  | 07-26 | cool        | sunny    | normal   | true  | no        |
  | 07-30 | mild        | sunny    | high     | false | yes       |

  ID3算法使用信息增益，在C4.5算法中引入了信息增益率(gainratio)。为什么呢？

  接着上面例子，假如使用第一列Day作为特征（注：任意过来一条天气数据，预估去打高尔夫还是不去打高尔夫，设为随机变量Y；设意过来一条数据的日期设为随机变量X，取值为07-05,07-06···07-30）。日期的信息增益为![H(Y|X)=0](https://private.codecogs.com/gif.latex?H%28Y%7CX%29%3D0)，日期的信息增益率为![g(Y,X)=0.9403](https://private.codecogs.com/gif.latex?g%28Y%2CX%29%3D0.9403)，这特征可真是够“好”的！显然这种特征对于样本的分隔没有任何意义。类似的情况还有人们的身份证号、信用卡号、学号等等特征。 

  那么导致这样的偏差的原因是什么呢？从上面的例子应该能够感受出来，原因就是该特征可以选取的值过多。解决办法自然就想到了如何能够对节点分支过多的情况进行惩罚，这样就引入了公式来表示属性A的内部信息。

  ## 3.1 属性A的内部信息（Intrinsic Information of an Attribute）

  ![IntI(Y,X)=-\sum_{i}^{ }\frac{\left | X_i \right |}{\left | Y \right |}log(\frac{\left | X_i \right |}{\left | Y \right |})](https://private.codecogs.com/gif.latex?IntI%28Y%2CX%29%3D-%5Csum_%7Bi%7D%5E%7B%20%7D%5Cfrac%7B%5Cleft%20%7C%20X_i%20%5Cright%20%7C%7D%7B%5Cleft%20%7C%20Y%20%5Cright%20%7C%7Dlog%28%5Cfrac%7B%5Cleft%20%7C%20X_i%20%5Cright%20%7C%7D%7B%5Cleft%20%7C%20Y%20%5Cright%20%7C%7D%29)

  注：![X](https://private.codecogs.com/gif.latex?X)表示某个属性![A](https://private.codecogs.com/gif.latex?A)的随机变量，![\left | X_i \right |](https://private.codecogs.com/gif.latex?%5Cleft%20%7C%20X_i%20%5Cright%20%7C)表示属性A的第![i](https://private.codecogs.com/gif.latex?i)个分类的个数，![\left | Y \right |](https://private.codecogs.com/gif.latex?%5Cleft%20%7C%20Y%20%5Cright%20%7C)表示样本的总个数，![\frac{\left | X_i \right |}{\left | Y \right |}](https://private.codecogs.com/gif.latex?%5Cfrac%7B%5Cleft%20%7C%20X_i%20%5Cright%20%7C%7D%7B%5Cleft%20%7C%20Y%20%5Cright%20%7C%7D)表示属性A的第![i](https://private.codecogs.com/gif.latex?i)个分类占样本总个数的比例。

  ## **3.2 信息增益率**

  ![g(Y|X)=\frac{g(Y,X)}{IntI(Y,X)}](https://private.codecogs.com/gif.latex?g%28Y%7CX%29%3D%5Cfrac%7Bg%28Y%2CX%29%7D%7BIntI%28Y%2CX%29%7D)

  注：就是在信息增益上又除了一个量，这个量能够起到惩罚的作用，**类别越多这个量越大**。

  (注：这里为了便于区分，重新命名了随机变量) 

  （1）Day日期的内部信息

  ![IntI(D,Day)=14\cdot (-\frac{1}{14}\cdot log_2(\frac{1}{14}))=3.8074](https://private.codecogs.com/gif.latex?IntI%28D%2CDay%29%3D14%5Ccdot%20%28-%5Cfrac%7B1%7D%7B14%7D%5Ccdot%20log_2%28%5Cfrac%7B1%7D%7B14%7D%29%29%3D3.8074)

  （2）Outlook天气的内部信息

  ![IntI(D,Outlook)=-\frac{8}{14} log_2(\frac{8}{14})-\frac{4}{14}log_2(\frac{4}{14})-\frac{2}{14}log_2(\frac{2}{14})=1.3788](https://private.codecogs.com/gif.latex?IntI%28D%2COutlook%29%3D-%5Cfrac%7B8%7D%7B14%7D%20log_2%28%5Cfrac%7B8%7D%7B14%7D%29-%5Cfrac%7B4%7D%7B14%7Dlog_2%28%5Cfrac%7B4%7D%7B14%7D%29-%5Cfrac%7B2%7D%7B14%7Dlog_2%28%5Cfrac%7B2%7D%7B14%7D%29%3D1.3788)

  （3）Day日期信息的增益率

  ![g(D|Day)=\frac{g(D,Day)}{IntI(D,Day)}=\frac{0.9403}{3.8074}=0.247](https://private.codecogs.com/gif.latex?g%28D%7CDay%29%3D%5Cfrac%7Bg%28D%2CDay%29%7D%7BIntI%28D%2CDay%29%7D%3D%5Cfrac%7B0.9403%7D%7B3.8074%7D%3D0.247)

  （4）Outlook天气的信息增益率

  ![g(D|Outlook)=\frac{g(D,Outlook)}{IntI(D,Outlook)}=\frac{0.3949}{1.3788}=0.2864](https://private.codecogs.com/gif.latex?g%28D%7COutlook%29%3D%5Cfrac%7Bg%28D%2COutlook%29%7D%7BIntI%28D%2COutlook%29%7D%3D%5Cfrac%7B0.3949%7D%7B1.3788%7D%3D0.2864)

- ID3，C4.5， C5.0

  C4.5 解决了ID3 由于特征的属性值过多的偏向性问题 比如ID，如果确定了一个ID，则信息增益奇高，但是ID对最后的结果其实是没有实际意义的。所以C4.5引入了信息增益率扣除内部信息的方法。

  ID3树与CART树的区别：

  - ID3算法中，选择的是信息增益来进行特征选择，信息增益大的特征优先选择。
  - 而在C4.5中，选择的是信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的缺点。
  - 但是无论是ID3还是C4.5，都是基于熵的模型，里面会涉及到大量的对数运算，能不能简化一下？

- gini系数

  参考文档: https://blog.csdn.net/weixin_41843918/article/details/90485566

  ![image-20201108201321373](D:\技能学习\bigdata\img\决策树资料收集\image-20201108201321373.png)

### 2.决策树的过程

**决策树通常有三个步骤**：特征选择、决策树的生成、决策树的修剪。

**.1 特征选择**

特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。

在特征选择中通常使用的准则是：信息增益。

**.2 决策树生成**

选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。

**.3 决策树剪枝**

剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。

- 决策树学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。
- 决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
- 决策树学习的损失函数：正则化的极大似然函数
- 决策树学习的测试：最小化损失函数
- 决策树学习的目标：在损失函数的意义下，选择最优决策树的问题。
- 决策树原理和问答猜测结果游戏相似，根据一系列数据，然后给出游戏的答案。

**使用决策树做预测需要以下过程：**

- 收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过参访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。
- 准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。
- 分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。
- 训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。
- 测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。
- 使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。



### 3.决策树的优缺点 

**.1 优点**

- 决策树易于理解和解释，可以可视化分析，容易提取出规则；
- 可以同时处理标称型和数值型数据；
- 比较适合处理有缺失属性的样本；
- 能够处理不相关的特征；
- 测试数据集时，运行速度比较快；
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

**.2 缺点**

- 容易发生过拟合（随机森林可以很大程度上减少过拟合）；
- 容易忽略数据集中属性的相互关联；
- 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。
- ID3算法计算信息增益时结果偏向数值比较多的特征。



### 4.计算决策树、算法案例





## 前置知识储备(K1，K2，K4)

对于ML来说是K1,K2,对我本人来说是K1,K2 + K4 ，掌握程度要到实践境

### 1. 熵和基尼杂质的区别







### 2.随机森林和梯度提升树的区别

由于决策树过度拟合的问题，所以生产环境很少直接使用决策树，一般是使用决策树的集成算法。





## 参考资料 

### 1.《Next-Generation Machine Learning with Spark: Covers XGBoost, LightGBM, Spark NLP, Distributed Deep Learning with Keras, and More》

https://learning.oreilly.com/library/view/next-generation-machine-learning/9781484256695/html/488426_1_En_3_Chapter.xhtml

![image-20201108004829349](D:\技能学习\bigdata\img\decisionTree.png)



### 随机森林的客户流失预测

随机森林是一种强大的整体学习算法，它基于多个决策树作为基础模型，每个决策树都并行地对不同的自举数据子集进行训练。如前所述，决策树倾向于过度拟合。随机森林通过使用称为*装袋* （引导聚合）的技术来训练随机抽取数据子集的每个决策树，从而解决了过度拟合问题。套袋减少了模型的差异，有助于避免过度拟合。随机森林减少模型的方差而不增加偏差。它还执行*功能袋装*，该功能为每个决策树随机选择功能。特征装袋的目的是减少单个树之间的相关性。

对于分类，最终类别由多数投票决定。各个决策树生成的类的模式（最频繁出现）成为最终类。为了进行回归，各个决策树的输出平均值将成为模型的最终输出。

由于随机森林使用决策树作为其基本模型，因此它继承了其大多数特性。它能够处理连续特征和分类特征，并且不需要特征缩放和一键编码。随机森林在不平衡数据上也表现出色，因为它的层次结构性质迫使它们同时处理这两个类。最后，随机森林可以捕获因变量和自变量之间的非线性关系。

随机森林由于其可解释性，准确性和灵活性，是用于分类和回归的最受欢迎的基于树的集成算法之一。但是，训练随机森林模型可能需要大量计算（这使其非常适合在多核和分布式环境（如Hadoop或Spark）上并行化）。与线性模型（例如逻辑回归或朴素贝叶斯）相比，它需要更多的内存和计算资源。同样，随机森林在高维数据（例如文本或基因组数据）上的表现也很差。



数据来源 ： Kaggle 

https://www.kaggle.com/becksddf/churn-in-telecoms-dataset （用google账号登陆）

数据已经下载到本地了，地址: H:\software\bigml_59c28831336c6604c800002a.csv

![image-20201108005746843](D:\技能学习\bigdata\img\kaggle.png)

















### 2.《Learning Spark》

https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html#tree_based_models

![image-20201108005918179](D:\技能学习\bigdata\img\decisionTree_learningSpark.png)